{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# os.environ['CUDA_VIDIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split_name='train', columns=['text', 'stars'], folder='data'):\n",
    "    '''\n",
    "        \"split_name\" may be set as 'train', 'valid' or 'test' to load the corresponding dataset.\n",
    "        \n",
    "        You may also specify the column names to load any columns in the .csv data file.\n",
    "        Among many, \"text\" can be used as model input, and \"stars\" column is the labels (sentiment). \n",
    "        If you like, you are free to use columns other than \"text\" for prediction.\n",
    "    '''\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"Success\")\n",
    "        return df\n",
    "    except:\n",
    "        print(f\"Failed loading specified columns... Returning all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "Success\n",
      "select [text, stars] columns from the valid split\n",
      "Success\n",
      "select [text, stars] columns from the test split\n",
      "Failed loading specified columns... Returning all columns from the test split\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text', 'stars'])\n",
    "valid_df = load_data('valid', columns=['text', 'stars'])\n",
    "# the test set labels (the 'stars' column) are not available! So the following code will instead return all columns\n",
    "test_df = load_data('test', columns=['text', 'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "# As an example, we only use the text data.\n",
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']\n",
    "\n",
    "x_valid = valid_df['text']\n",
    "y_valid = valid_df['stars']\n",
    "\n",
    "x_test = test_df['text']\n",
    "\n",
    "x_train_processed = pd.DataFrame({'text':x_train, 'label':y_train})\n",
    "x_valid_processed = pd.DataFrame({'text':x_valid, 'label':y_valid})\n",
    "# x_valid_processed.to_csv('data_processed/train.csv', index=None)\n",
    "# x_valid_processed .to_csv('data_processed/valid.csv', index=None)\n",
    "train_dataset = Dataset.from_pandas(x_train_processed)\n",
    "valid_dataset = Dataset.from_pandas(x_valid_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(y_valid.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = {\n",
    "#     'train': 'data_processed/train.csv',\n",
    "#     'valid': 'data_processed/valid.csv'\n",
    "# }\n",
    "# dataset = load_dataset('csv', data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"I've been here a handful of times now and I've never been disappointed.  The food is always good and the servers are quick.   So far my two favorite items are the Peppersauce Burger with pastrami and the Peppersauce Patty.  Even as I type this my mouth is watering and I just had the Peppersauce Burger.  \\n\\nThe burgers are well done and still juicy!  I always leave stuffed and happy.  The burgers can be a little on the greasy side, need two or three napkins.  I've also had them when you only needed on napkin to clean up.  Either way it was still tasty!\\n\\nI've seen a couple of people get salads and they are huge and look good.\\n\\nThe servers have always been friendly even when it was really busy.\",\n",
       "  'The service was terrible. The food was just ok. Dessert was the best part of the whole experience.',\n",
       "  'Alil pricey for the location but completly get the bang for your buck sweet fries on point 100%',\n",
       "  \"Don't get your car washed here. Paid 11 and my car came out covered in some sort of film/dust. The cashier was nice enough to give me a refund, but I wish I didn't waste my time.\",\n",
       "  \"Cute but tight. Not expensive and creative. I love the place, the decoration makes you feel at home. Bread it's good and coffee is really tasty. It's one of my favorite places in Montreal!\"],\n",
       " 'label': [5, 1, 4, 1, 5]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to Load the model\n",
    "\n",
    "In this pipeline, I try to use bert models from HuggingFace to do the test classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/bert-base-cased/resolve/main/vocab.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dingdongliu/Documents/COMP4332/COMP4332-2022Spring/Project 1/BART_MODEL.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dingdongliu/Documents/COMP4332/COMP4332-2022Spring/Project%201/BART_MODEL.ipynb#ch0000044?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:546\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=543'>544</a>\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=544'>545</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=545'>546</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=546'>547</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=547'>548</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1763\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1760'>1761</a>\u001b[0m                 resolved_vocab_files[file_id] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1761'>1762</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1762'>1763</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1764'>1765</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1765'>1766</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1766'>1767</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1767'>1768</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1768'>1769</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1724\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1721'>1722</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1722'>1723</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1723'>1724</a>\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1724'>1725</a>\u001b[0m             file_path,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1725'>1726</a>\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1726'>1727</a>\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1727'>1728</a>\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1728'>1729</a>\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1729'>1730</a>\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1730'>1731</a>\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1731'>1732</a>\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1732'>1733</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1734'>1735</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1735'>1736</a>\u001b[0m         \u001b[39mif\u001b[39;00m local_files_only:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py:1921\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1916'>1917</a>\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1918'>1919</a>\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1919'>1920</a>\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1920'>1921</a>\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1921'>1922</a>\u001b[0m         url_or_filename,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1922'>1923</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1923'>1924</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1924'>1925</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1925'>1926</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1926'>1927</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1927'>1928</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1928'>1929</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1929'>1930</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1930'>1931</a>\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1931'>1932</a>\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=1932'>1933</a>\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py:2125\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2122'>2123</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2123'>2124</a>\u001b[0m     r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mhead(url, headers\u001b[39m=\u001b[39mheaders, allow_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, proxies\u001b[39m=\u001b[39mproxies, timeout\u001b[39m=\u001b[39metag_timeout)\n\u001b[0;32m-> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2124'>2125</a>\u001b[0m     _raise_for_status(r)\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2125'>2126</a>\u001b[0m     etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2126'>2127</a>\u001b[0m     \u001b[39m# We favor a custom header indicating the etag of the linked resource, and\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2127'>2128</a>\u001b[0m     \u001b[39m# we fallback to the regular etag header.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2128'>2129</a>\u001b[0m     \u001b[39m# If we don't have any of those, raise an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py:2052\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2048'>2049</a>\u001b[0m     \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRevisionNotFound\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2049'>2050</a>\u001b[0m         \u001b[39mraise\u001b[39;00m RevisionNotFoundError((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m404 Client Error: Revision Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mrequest\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/file_utils.py?line=2051'>2052</a>\u001b[0m request\u001b[39m.\u001b[39;49mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/requests/models.py:960\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/requests/models.py?line=956'>957</a>\u001b[0m     http_error_msg \u001b[39m=\u001b[39m \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code, reason, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl)\n\u001b[1;32m    <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/requests/models.py?line=958'>959</a>\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> <a href='file:///home/dingdongliu/miniconda3/envs/venv/lib/python3.10/site-packages/requests/models.py?line=959'>960</a>\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/bert-base-cased/resolve/main/vocab.txt"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset_tokenized = train_dataset_tokenized.remove_columns(['text'])\n",
    "train_dataset_tokenized = train_dataset_tokenized.rename_column(\"label\", \"labels\")\n",
    "train_dataset_tokenized.set_format('torch')\n",
    "\n",
    "valid_dataset_tokenized = valid_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset_tokenized = valid_dataset_tokenized.remove_columns(['text'])\n",
    "valid_dataset_tokenized = valid_dataset_tokenized.rename_column(\"label\", \"labels\")\n",
    "valid_dataset_tokenized.set_format('torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_dataset_tokenized.shuffle(seed=42).select(range(500))\n",
    "small_valid_dataset = valid_dataset_tokenized.shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del pytorch_model\n",
    "# del trainer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=4)\n",
    "valid_dataloader = DataLoader(small_valid_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels=5)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(name='linear', optimizer=optimizer,\n",
    "                             num_warmup_steps=0, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=5)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset_tokenized = valid_dataset.map(tokenize_function, batched=True)\n",
    "small_train_dataset = train_dataset_tokenized.shuffle(\n",
    "    seed=42).select(range(1000))\n",
    "small_valid_dataset = valid_dataset_tokenized.shuffle(\n",
    "    seed=42).select(range(200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset_tokenized = valid_dataset.map(tokenize_function, batched=True)\n",
    "small_train_dataset = train_dataset_tokenized.shuffle(\n",
    "    seed=42).select(range(1000))\n",
    "small_valid_dataset = valid_dataset_tokenized.shuffle(\n",
    "    seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = small_train_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=2,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = small_valid_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample code from the hugging face website\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
    "# the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tokenized[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93da902caf8eaffdcca2eca9e679b93ff3b8cd0049d761505e7d28863434ff18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
