{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting random seeds to replicate results easily\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tensorflow.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "In this tutorial, RMSE is used to evaluate the performance of a model. A model that reports lower RMSE indicates that it is a better model and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, actual):\n",
    "    '''\n",
    "    params:\n",
    "        pred <np.array>: an array containing all predicted ratings\n",
    "        actual <np.array>: an array containing all ground truth ratings\n",
    "\n",
    "    return:\n",
    "        a scalar whose value is the rmse\n",
    "    '''\n",
    "    # Ignore ratings with value zero.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return np.sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Learning (WDL) Model Implementation\n",
    "The Wide and Deep Learning Model can be divided into two parts, the \"wide\" part and the \"deep\" part.\n",
    "\n",
    "**The wide component** is a generalized linear model that takes in the raw input features and the cross-product transformation of categorical features, which enables it to **learn the co-occurrence patterns of items or features**.\n",
    "\n",
    "**The deep component** is a Feed-forward Neural Network (FNN) which takes in both continuous and categorical features as input. Specifically, the normalized values of continuous features are concatenated with the low-dimensional dense embedding vectors converted from categorical features. This concatenated vector is then fed into the FNN during each foward pass. This mechanism tends to **increase the diversity of recommendations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wdl_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    '''\n",
    "    params:\n",
    "        len_continuous: number of continuous features\n",
    "        deep_vocab_lens: an array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep categorical feature\n",
    "        len_wide: number of wide features\n",
    "        embed_size: dimension of the embedding vectors of deep categorical features\n",
    "\n",
    "    return:\n",
    "        a keras Model object for the constructed wdl model \n",
    "    '''\n",
    "    # A list containing all input layers\n",
    "    input_list = []\n",
    "    \n",
    "    # Input layer for continuous features\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
    "    input_list.append(continuous_input)\n",
    "\n",
    "    # Get embeddings for all deep categorical features\n",
    "    emb_list = []\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        _input = Input(shape=(1,), dtype='int32')\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
    "        _emb = Reshape((embed_size,))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "\n",
    "    # Create input layer for deep component by concatenating the embeddings and continuous features' input layer\n",
    "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
    "\n",
    "    # Construct deep component\n",
    "    dense_1 = Dense(256, activation='relu')(deep_input)\n",
    "    dense_1_dp = Dropout(0.3)(dense_1)\n",
    "    dense_2 = Dense(128, activation='relu')(dense_1_dp)\n",
    "    dense_2_dp = Dropout(0.3)(dense_2)\n",
    "    dense_3 = Dense(64, activation='relu')(dense_2_dp)\n",
    "    dense_3_dp = Dropout(0.3)(dense_3)\n",
    "\n",
    "    # Create input layer for wide component\n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
    "    input_list.append(wide_input)\n",
    "\n",
    "    # Concatenate the outputs of deep and wide components and feed the concatenated vector into the finall fully connected layer\n",
    "    fc_input = Concatenate()([dense_3_dp, wide_input])\n",
    "    model_output = Dense(1)(fc_input)\n",
    "\n",
    "    model = Model(inputs=input_list, outputs=model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Below are utility functions that helps us retrieve the numerical values of different features from the dataset, and generate combinations of features to be used by the WDL model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_features(df, continuous_columns):\n",
    "    '''\n",
    "    params:\n",
    "        df: input dataframe\n",
    "        continuous_columns: column names of continuous features\n",
    "\n",
    "    return: \n",
    "        a numpy array where each row contains the values of continuous features in the corresponding row of the input dataframe\n",
    "    '''\n",
    "    continuous_features = df[continuous_columns].values\n",
    "    return continuous_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features Cross Product Transformation\n",
    "This function is used to generate a variety of feature combinations that occurred frequently in the dataset.\n",
    "\n",
    "For example, the following items occurred frequently in the dataset:\n",
    "\n",
    "|Item Name|Occurrence|\n",
    "|---|---|\n",
    "|A|4|\n",
    "|B|3|\n",
    "|C|2|\n",
    "|D|1|\n",
    "\n",
    "If we set `topk=3`, it means that we will only consider the top 3 items with the highest occurrence to generate combinations.\n",
    "\n",
    "If we set `comb_p=2`, it means that we will generate combinations with 2 items in each generated combinations.\n",
    "\n",
    "In this case, the following combinations will be generated: `[('A', 'B'), ('A', 'C'), ('B', 'C')]`\n",
    "\n",
    "Test code: `get_top_k_p_combinations(pd.DataFrame({'item_categories': ['A, B, C, D', 'A, B, C', 'A, B', 'A']}), comb_p=2, topk=3, output_freq=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    '''\n",
    "    params:\n",
    "        df: input dataframe\n",
    "        comb_p: number of elements in each combination (e.g., there are two elements in the combination {fried chicken, chicken and waffle}, and three elements in the combination {fried chicken, chicken and waffle, chicken fried rice})\n",
    "        topk: number of most frequent combinations to retrieve\n",
    "        output_freq: whether to return the frequencies of retrieved combinations\n",
    "\n",
    "    return:\n",
    "        1. output_freq = True: a list X where each element is a tuple containing a combination tuple and corresponding frequency, and the elements are stored in the descending order of their frequencies\n",
    "        2. output_freq = False: a list X where each element is a tuple containing a combination tuple, and the elements are stored in the descending order of their frequencies\n",
    "    '''\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "        return list(combinations(categories, comb_p))\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Wide Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_features(df, selected_categories_to_idx, top_combinations):\n",
    "    '''\n",
    "    params:\n",
    "        df: input dataframe\n",
    "        selected_categories_to_idx: a dictionary mapping item categories to corrresponding integral indices\n",
    "        top_combinations: a list containing retrieved mostly frequent combinantions of item categories\n",
    "\n",
    "    return:\n",
    "        a numpy array where each row contains the categorical features' binary encodings and cross product transformations for the corresponding row of the input dataframe\n",
    "    '''\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "        return binary_output\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    category_cross_transform_features = np.array(df.item_categories.apply(lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "    return np.concatenate((category_binary_features, category_cross_transform_features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratings Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading train, validation and test rating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/valid.csv\")\n",
    "te_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "tr_ratings = tr_df['stars'].values\n",
    "val_ratings = val_df['stars'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading content feautures tables of users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"data/user.csv\", index_col=0)\n",
    "item_df = pd.read_csv(\"data/business.csv\", index_col=0)\n",
    "\n",
    "# Renaming columns by adding prefixes to column names\n",
    "user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
    "item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding the table by using user_id and business_id\n",
    "Expanding the train, valiation and test dataset by using `user_id` and `business_id` to query more features from `user_df` and `item_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').reset_index(drop=True)\n",
    "val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').reset_index(drop=True)\n",
    "te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns containing conitnuous features\n",
    "continuous_columns = [\"user_average_stars\", \"user_cool\", \"user_fans\", \n",
    "                      \"user_review_count\", \"user_useful\", \"user_funny\",\n",
    "                      \"item_is_open\", \"item_latitude\", \"item_longitude\", \n",
    "                      \"item_review_count\", \"item_stars\"]\n",
    "\n",
    "# Get values of continous features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
    "val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
    "te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
    "\n",
    "# Standardize each feature by removing the mean of the training samples and scaling to unit variance.\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html for more details.\n",
    "scaler = StandardScaler().fit(tr_continuous_features)\n",
    "\n",
    "tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "val_continuous_features = scaler.transform(val_continuous_features)\n",
    "te_continuous_features = scaler.transform(te_continuous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing deep categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepcify column names of deep categorical features\n",
    "item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n",
    "\n",
    "# An array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep categorical feature\n",
    "# Transforming words into indices for each categorical columns\n",
    "item_deep_vocab_lens = []\n",
    "for col_name in item_deep_columns:\n",
    "    # Getting unique values of this deep categorical feature\n",
    "    unique_values = item_df[col_name].unique()\n",
    "    \n",
    "    # Creating a dictionary to map from unique values to the corresponding index\n",
    "    vocab = dict(zip(unique_values, range(1, len(unique_values)+1)))\n",
    "    \n",
    "    # Getting the number of unique values of this deep categorical features\n",
    "    item_deep_vocab_lens.append(len(vocab)+1)\n",
    "    \n",
    "    # Creating a new column where each entry stores the index of this deep categorical feature's value in the same row\n",
    "    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x])\n",
    "\n",
    "\n",
    "# Creating a dictionary mapping each business id to corresponding values of deep categorical features ('business_id' -> ['item_city_idx', 'item_postal_code_idx', 'item_state_idx'] in this case)\n",
    "item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "item_to_deep_categorical_features = dict(zip(item_df['business_id'].values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "\n",
    "# Creating numpy arrays storing corresponding deep categorical features' values of train/validation/test sets using the above mapping\n",
    "tr_deep_categorical_features = np.array(tr_df['business_id'].apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "val_deep_categorical_features = np.array(val_df['business_id'].apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "te_deep_categorical_features = np.array(te_df['business_id'].apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing wide features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing binary encoding for each selected category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categories of all items \n",
    "all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "\n",
    "# Sort all unique values of the item categories by their frequencies in descending order\n",
    "category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top 500 most frequent categories\n",
    "selected_categories = [t[0] for t in category_sorted[:500]]\n",
    "\n",
    "# Create a dictionary mapping each secleted category to a unique integral index\n",
    "selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "\n",
    "# Map all categories unseen in the item df to index 0\n",
    "selected_categories_to_idx['unk'] = 0\n",
    "\n",
    "# Create a dictionary mapping each integral index to corresponding category\n",
    "idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing cross product transformation for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent categories combinantions using the utility function defined previously and store them in the folloing list\n",
    "top_combinations = []\n",
    "\n",
    "# Get top 50 most frequent two-categories combinantions in the train set\n",
    "\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 2, 50, output_freq=False)\n",
    "\n",
    "# Get top 30 most frequent three-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n",
    "\n",
    "# Get top 20 most frequent four-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n",
    "\n",
    "# Convert each combinantion in the list to a set data structure\n",
    "top_combinations = [set(t) for t in top_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting values of wide features for train/validation/test sets using the utility function defined previously\n",
    "# The following matrices should have a shape of (n_samples, len(selected_categories_to_idx)+len(top_combinations))\n",
    "tr_wide_features = get_wide_features(tr_df, selected_categories_to_idx, top_combinations)\n",
    "val_wide_features = get_wide_features(val_df, selected_categories_to_idx, top_combinations)\n",
    "te_wide_features = get_wide_features(te_df, selected_categories_to_idx, top_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating continuous features, deep categorical features and wide features as an input list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_features = []\n",
    "tr_features.append(tr_continuous_features)\n",
    "tr_features += [tr_deep_categorical_features[:, i] for i in range(tr_deep_categorical_features.shape[1])]\n",
    "tr_features.append(tr_wide_features)\n",
    "\n",
    "val_features = []\n",
    "val_features.append(val_continuous_features)\n",
    "val_features += [val_deep_categorical_features[:, i] for i in range(val_deep_categorical_features.shape[1])]\n",
    "val_features.append(val_wide_features)\n",
    "\n",
    "te_features = []\n",
    "te_features.append(te_continuous_features)\n",
    "te_features += [te_deep_categorical_features[:, i] for i in range(te_deep_categorical_features.shape[1])]\n",
    "te_features.append(te_wide_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the WDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 04:35:40.087537: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-06 04:35:40.089698: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-06 04:35:40.090155: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "wdl_model = build_wdl_model(len(tr_continuous_features[0]), item_deep_vocab_lens,  len(tr_wide_features[0]), embed_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using Adagrad optimizer and mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 9s 3ms/step - loss: 2.7645\n"
     ]
    }
   ],
   "source": [
    "wdl_model.compile(optimizer='adagrad', loss='mse')\n",
    "\n",
    "history = wdl_model.fit(\n",
    "        tr_features, \n",
    "        tr_ratings, \n",
    "        epochs=1, verbose=1, callbacks=[ModelCheckpoint('models/model.h5')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on train and validation sets using RMSE¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  1.238755676383003\n",
      "VALID RMSE:  1.2352477734807417\n"
     ]
    }
   ],
   "source": [
    "y_pred = wdl_model.predict(tr_features)\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = wdl_model.predict(val_features)\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
